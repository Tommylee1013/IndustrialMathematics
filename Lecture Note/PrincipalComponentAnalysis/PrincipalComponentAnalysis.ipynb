{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "Why is dimensionality reduction necessary?\n",
    "\n",
    "- To make large problems computationally efficient (conserving computation, storage and network resources)\n",
    "- To improve the quality of data mining results\n",
    "    - Improve classification accuracy or clustering modularity\n",
    "    - Reduce the amount of training data needed to obtain a desired level of performance\n",
    "\n",
    "A simplified taxonomy of dimensionality reduction techniques\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc3c290941b86f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Why PCA?\n",
    "\n",
    "**Feature Extraction**\n",
    "\n",
    "- Extract useful features among a large number of features\n",
    "\n",
    "**Data Compression**\n",
    "\n",
    "- Efficient storage and retieval\n",
    "\n",
    "**Vidualization**\n",
    "\n",
    "- Projection of High-dimensional data onto 2D or 3D\n",
    "\n",
    "to find a set orthogonal bases to preserve the variance of the original data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e974bc1808a2909"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PCA : Principal Component Analysis\n",
    "\n",
    "projection onto a basis\n",
    "\n",
    "$$\\vec{b} - p\\vec{a} = 0 \\Rightarrow \\vec{b}^T \\vec{a} - p\\vec{a}^T\\vec{a}= 0 \\Rightarrow p = \\frac{\\vec{b}^T\\vec{a}}{\\vec{a}^T\\vec{a}}$$\n",
    "\n",
    "$$\\vec{x} = p\\vec{a} = \\frac{\\vec{b}^T\\vec{a}}{\\vec{a}^T\\vec{a}}$$\n",
    "\n",
    "if $\\vec{a}$  is unit vector\n",
    "\n",
    "$$p = \\vec{b}^T\\vec{a} \\Rightarrow \\vec{x} = p\\vec{a} = (\\vec{b}^T\\vec{a})\\vec{a}$$\n",
    "\n",
    "**Covariance**\n",
    "\n",
    "- $\\mathbf{X}$ : a data set (m by n, m : number of variables, n : number of records)\n",
    "\n",
    "$$\\mathrm{Cov}(\\mathbf{X}) = \\frac{1}{n}(\\mathbf{X} - \\bar\\mathbf{X})(\\mathbf{X} - \\bar\\mathbf{X})^T$$\n",
    "\n",
    "- $\\mathrm{Cov}(\\mathbf{X})_{ij} = \\mathrm{Cov}(\\mathbf{X})_{ji}$\n",
    "- Total variance of the data set\n",
    "\n",
    "$$\\mathrm{tr} [\\mathrm{Cov}(\\mathbf{X})]$$\n",
    "\n",
    "$$= \\mathrm{Cov}(\\mathbf{X})_{11} + \\mathrm{Cov}(\\mathbf{X})_{22} + \\mathrm{Cov}(\\mathbf{X})_{33} + \\dots + \\mathrm{Cov}(\\mathbf{X})_{mm}$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec341b4e46dce286"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Eigenvalue and eigenvector**\n",
    "\n",
    "The matrix $\\mathbf{A}$ is given, scalar value $\\lambda$ and vector $\\mathbf{x}$ that satisfy $\\mathbf{Ax} = \\lambda \\mathbf{x} $ or $(\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{x} = \\mathbf{0}$ are called **eigenvalue** and **eigenvector**, respectively.\n",
    "\n",
    "if a matrix A is an m by n diagonalizable matrix, there exist m eigenvalues and eigenvectors, and eigenvectors are orthogonal to each other.\n",
    "\n",
    "$$\\mathrm{tr}(\\mathbf{A}) = \\lambda_1 + \\lambda_2 + \\lambda_3 + \\dots + \\lambda_m$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad557ae77f4fcab6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
